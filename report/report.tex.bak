\documentclass[CJK,compress,t]{beamer}
\usepackage{CJK}
\usepackage{float}

\usetheme{umbc4}

\setbeamerfont{documenttitle}{size=\Huge}
\setbeamerfont{footnote}{size=\huge}

\begin{document}
\begin{CJK}{GBK}{hei}
\baselineskip=16pt

\title{ 基于改进语言模型的网页排序问题研究 }
\author{ 杨波 }
\institute{
\begin{tabular}{r @{：} l}
指导教师 & 黄亚楼教授 \\
研究方向 & 数据挖掘 \\
\end{tabular}
}
\date{\today}


\setfootline{ {Intellegent Information Processing Lab} \hfill  \insertframenumber/\inserttotalframenumber}

\begin{frame}
\titlepage
\end{frame}


\begin{frame}
\frametitle{概要}
\begin{block}
{\LARGE \tableofcontents}
\end{block}
\end{frame}

\section{题目阐述}
\begin{frame}
\frametitle{概要}
\begin{block}
{\LARGE \tableofcontents[currentsection]}
\end{block}
\end{frame}

\begin{frame}
\frametitle{题目阐述}
%\begin{table}[t]
\large
\renewcommand{\arraystretch}{2}
\begin{tabular}{r @{：} l}
论文题目 & 基于改进语言模型的网页排序问题研究 \\
关键词 & 信息检索、语言模型、网页排序、变长多元组 \\
研究领域 & 信息检索领域 \\
关注问题 & 信息检索中网页排序问题 \\
论文目标 & 对经典语言模型进行改进，提高排序性能 \\
\end{tabular}
%\end{table}
\end{frame}

\section{选题依据}
\begin{frame}
\frametitle{概要}
\begin{block}
{\LARGE \tableofcontents[currentsection]}
\end{block}
\end{frame}


\begin{frame}
\frametitle{选题依据-背景}

{\Large 信息检索}
\begin{itemize}
\item 随着互联网的普及，搜索已经在人们的生活中占有了重要的地位
\item 搜索结果的排序是信息检索中关键的问题
\end{itemize}

\pause
{\Large 信息检索模型}
\begin{itemize}
\item 布尔模型
\item 向量空间模型
\item 概率检索模型(TF*IDF, BM25)
\item 统计语言检索模型(LMIR)
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{选题依据-统计语言模型}
{\Large 统计语言模型的历史}
\begin{itemize}
\item 来自speech recognization领域
\item 用来估计语音序列中下一个词的概率
\[
p(w|h_1, h_2, \ldots, h_n)
\]
\end{itemize}
\pause
{\Large basic n-gram}
\[
p(w_i|w_{i-n}...w_{i-1}) = \frac{C(w_{i-n}...w_{i-1}, w_i)}{C(w_{i-n}...w_{i-1})}
\]
\end{frame}

\begin{frame}
\frametitle{选题依据-统计语言模型}
{\Large 各种各样的语言模型}
\begin{itemize}
\item topic-based n-gram
\[
P(w_i|w_{i-n+1}...w_{i-1}) = \sum_{Topic=i}{\lambda_iP_i(w_i|w_{i-n+1}...w_{i-1})}
\]
\item skip-based n-gram
\[
P_{Mixed}(w_i|w_{i-n+1}...w_{i-1}) = \sum^{n-1}_{k=1}{\lambda_k \times P(w_i | w_{i-k})}
\]
\item 指数级语言模型
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{选题依据-统计语言模型}

{\Large 信息检索中的语言模型}
\begin{itemize}
\item 查询产生相应文档的概率
\begin{center}
$p(d\ is\ relevent|q) = \frac{p(q|d\ is\ relevent)p(d\ is\ relevent)}{p(q)}$
\end{center}
\item 信息检索中的语言模型
\begin{center}
$p(d|q) \propto p(q|d)p(d)$
\end{center}
\begin{center}\footnotesize 其中d是待排序文档, q用户的查询词, 返回的文档以$p(d|q)$大小排序\end{center}
\item 关键在于估计$p(q|d)$和$p(d)$
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{选题依据-统计语言模型}
{\Large 估计$p(q|d)$}
\[
log \; p(q|d) = \sum_{i=1}^{n}{log \; p(w_{i}|d)}
\]
\begin{center}\footnotesize 其中$w_i$为用户查询中的单词\end{center}

{\Large 计算$p(w_i|d)$}
\[
p(w|d) = \frac{c(w;d)}{\sum_{w^{'} \in V}{c(w^{'};d)}}
\]
\begin{center}\footnotesize 其中$c(w;d)$为文档d中出现单词w的次数；$V$为词汇表\end{center}

\end{frame}


\begin{frame}
\frametitle{选题依据-统计语言模型}
{\Large 对未见词进行估计}
\begin{itemize}
\item 未见词会造成0概率问题

\item 通过使用候选数据集做backoff，即在网页中有未出现的查询词的时候，使用平滑数据集中的词频代替网页的词频

\item 通过减少文档中可见词的概率来平滑语言模型
    \begin{itemize}
    \item The Jelinek-Mercer method
    %\[
    %        p_\lambda(w|d) = (1-\lambda)p(w|d)+\lambda p(w|C)
    %\]
    \item Bayesian smoothing using Dirichlet priors
    \item Absolute discounting
    \item Two-stage Smoothing model
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{选题依据-统计语言模型}
{\Large 存在问题}
\begin{itemize}
\item 简单的统计网页中出现\underline{每个单词}的频率会受到网页中各种噪声词语的干扰
%\item 将网页这种超文本当做纯文本来处理，网页中的某些HTML模式可以用来更好的计算$p(q|d)$
\item 直接使用整个互联网的corpus去平滑文档中未出现的词，忽略了网页所属的网站以及目录带来的信息量
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{选题依据-统计语言模型}
{\Large 估计$p(d)$}
\begin{itemize}
\item 假设$p(d)$为均匀分布
\item 使用PageRank来做$p(d)$(Lemur)
\end{itemize}
\pause
{\Large 存在问题：无法利用更多的结构信息}
    \begin{itemize}
    \item 文档长度、URL长度
    \item 指向文档的链接数、PageRank值、HITS值
    \item ...
    \end{itemize}
\end{frame}


\section{研究内容}
\begin{frame}
\frametitle{概要}
\begin{block}
{\LARGE \tableofcontents[currentsection]}
\end{block}
\end{frame}

\begin{frame}
\frametitle{研究内容}
{\Large 语言模型的分隔}
\[
p(d|q) = p(q|d)p(d)
\]
\begin{itemize}
\item $p(q|d)$和query词相关，称为\underline{内容模型}
\item $p(d)$和query词无关，称为\underline{结构模型}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{研究内容}
{\Large 主要研究工作}
\begin{itemize}
\item 可变多元组内容生成算法研究(Variable N-gram)
\item 多层数据集平滑算法研究
\item 结构模型估计算法研究
\item 将以上方法在语言模型的框架下整合
\end{itemize}
\end{frame}

\section{研究方法}
\begin{frame}
\frametitle{概要}
\begin{block}
{\LARGE \tableofcontents[currentsection]}
\end{block}
\end{frame}

\begin{frame}
\frametitle{研究方法-可变多元组内容生成算法研究}
{\Large 1-gram}
\[
log \; p(q|d) = \sum_{i=1}^{n}{log \; p(w_{i}|d)}
\]
{\Large 问题}
\begin{table}
\caption{\scriptsize 相关文档中平均包含1-gram的个数}
\begin{center}
\begin{tabular}{cc}

数据集 & 平均包含1-gram个数 \\
\hline
HP2004 & 13.5 \\
NP2004 & 22.1 \\
TD2004 & 7.6 \\
\end{tabular}
\end{center}
\end{table}

\end{frame}

\begin{frame}
\frametitle{研究方法-可变多元组内容生成算法研究}
{\Large 问题-续}
\begin{table}
\caption{\scriptsize 只考虑1-gram时，被错误计算的文档的个数}
\begin{center}
\begin{tabular}{ccc}

数据集 & 平均文档个数 & 百分比 \\
\hline
HP2004 & 8998 & 12.1\% \\
NP2004 & 5603 & 7.6\% \\
TD2004 & 12563 & 17.2\% \\
\end{tabular}
\end{center}
\end{table}
\end{frame}

\begin{frame}
\frametitle{研究方法-可变多元组内容生成算法研究}
{\Large 考虑2-gram和3-gram的情况}
\begin{table}
\caption{\scriptsize 考虑2-gram时，被错误计算的文档的个数}
\begin{center}
\begin{tabular}{ccc}

数据集 & 平均文档个数 & 百分比 \\
\hline
HP2004 & 4006 & 5.4\% \\
NP2004 & 2861 & 3.9\% \\
TD2004 & 3674 & 5.1\% \\
\end{tabular}
\end{center}
\end{table}

\begin{table}
\caption{\scriptsize 考虑3-gram时，被错误计算的文档的个数}
\begin{center}
\begin{tabular}{ccc}

数据集 & 平均文档个数 & 百分比 \\
\hline
HP2004 & 246 & 0.3\% \\
NP2004 & 351 & 0.4\% \\
TD2004 & 13 & 0.02\% \\
\end{tabular}
\end{center}
\end{table}
\end{frame}

\begin{frame}
\frametitle{研究方法-可变多元组内容生成算法研究}
{\Large 多元组的影响}

随着纳入考虑的元组数目增多，包含相应元组数目的文档成为相关文档的概率也跟随增大
\pause
\begin{figure}[H]
\includegraphics[width=1.5in, height=1.5in]{images/n-gram-td2004.png}
\includegraphics[width=1.5in, height=1.5in]{images/n-gram-hp2004.png}
\includegraphics[width=1.5in, height=1.5in]{images/n-gram-np2004.png}
\end{figure}
{\scriptsize 横坐标：1元组，2元组，3元组
\ \ \ \ \ \ 纵坐标：包含了相应元组的文档中相关文档的比例
}
\end{frame}


\begin{frame}
\frametitle{研究方法-可变多元组内容生成算法研究}
{\Large 一个更准确合理的估计}
\[
log \; s(q|d) = \sum_{i=1}^{n}{log \; p(w_{1} \textrm{\ldots} w_{k}|d)}
\]
\begin{center}{\footnotesize k：query中单词的个数}\end{center}
{\Large 问题}
\begin{itemize}
\item 数据过于稀疏，造成所有$p(q|d)$都为0
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{研究方法-可变多元组内容生成算法研究}
{\Large 变长多元组}
\begin{itemize}
\item 同时考虑1元组、2元组和3元组
\item 为大的元组赋予较大的权重

\[
p(q|d) = \alpha_1 p(q_1|d) + \alpha_2 p(q_2|d) + ... + \alpha_n p(q_n|d)
\]
\begin{center}
\footnotesize 其中$q_1$\ldots$q_n$分别表示查询中的1元组到n元组，$\alpha_1$\ldots$\alpha_n$
为相应的权重
\end{center}
\end{itemize}
\pause
{\Large 新的问题}
\begin{itemize}
\item 相应的权重如何计算，怎么从训练样本中估计
\item 是否需要保证$p(q|d)$仍然是一个概率分布,如何保证$p(q|d)$是一个概率分布
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{研究方法-多层数据集平滑算法研究}
{\Large URL的意义}
\begin{center}
http://host.part/directory/part/file-name
\end{center}
{\Large 数据集定义}
\begin{itemize}
\item 目录数据集

URL中目录部分相同的文档，所组成的数据集，如:

http://www.nasa.gov/missions/future/  下的所有文档

\item 站点数据集

属于同一个站点的文档，所组成的数据集

\item 互联网数据集

整个.GOV所有文档，组成的数据集
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{研究方法-多层数据集平滑算法研究}
{\Large 线性插值平滑}
\[
p(w|d) = (1-\lambda-\mu-\omega)p(w|d) + \lambda p(w|D) + \mu p(w|S) + \omega p(w|C)
\]
\begin{center}
\footnotesize
其中，D为当前文档（网页）所在的目录中所有单词组成的数据集，S为对应的站点的数据集，C为整个互联网的数据集
\end{center}
\pause
{\Large 待解决的问题}
\begin{itemize}
\item 参数$\lambda, \mu, \omega$的估计
\item 简单线性插值是否适合多层数据集的平滑，考虑Dirichlet插值和其他更复杂的插值的算法
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{研究方法-结构模型估计算法研究}
{\Large 各种先验概率之间的关系}
\begin{figure}
\includegraphics[width=2.5in, height=2.5in]{images/structure.png}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{研究方法-结构模型估计算法研究}
{\Large 贝叶斯信念网络估计}

假设：每一个先验概率特征为$x_i$

已知：

\begin{itemize}
\item $p(x_i|d)$，即相关文档出现特征$x_i$的概率
\item $p(x_i|x_j)$，即不同的特征i,j统计不独立时，二者之间的条件概率
\end{itemize}

求解：

\begin{itemize}
\item $p(d|x_1, x_2, ..., x_n)$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{研究方法-实验及平台}
{\Large 实验平台}
\begin{itemize}
\item 数据集：.GOV文本数据集
\item 算法测试数据集：Letor 3.0 Topic Distillation, Home Page Finding, Named Page Finding
\item 评价指标：Precision@10, Recall@10, MAP, NDCG@10
\item 参考程序：Lemur平台
\item 程序环境：MSYS脚本系统，Windows 2003 Server， Linux
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{研究方法-实验及平台}
{\Large 主要实验}
\begin{itemize}
\item 改写Lemur，创建适用于计算变长多元组内容生成概率和多层数据集平滑算法的索引结构
\item 在新的索引结构上，完成变成多元组算法，对比不同的参数的效果和不同的参数估计方法的效果
\item 在变长多元组算法中加入平滑算法，对比不同的参数对算法性能的影响
\item 设计实现结构模型生成算法，对各种结构先验概率的效果进行分析
\item 综合计算改进后的语言模型，对其性能就行评价
\end{itemize}
\end{frame}

\section{总结与展望}
\begin{frame}
\frametitle{概要}
\begin{block}
{\LARGE \tableofcontents[currentsection]}
\end{block}
\end{frame}

\begin{frame}
\frametitle{总结与展望-工作总结}
{\Large 总结}
\begin{itemize}
\item 通过对现有语言模型研究的调研，分别提出针对内容模型生成算法，平滑算法和结构模型$p(d)$的改进算法

\pause
\item 通过实验说明了基于variable n-gram的内容模型生成算法和多层数据集平滑算法的可行性和必要性

%\pause
%\item 提出了基于朴素贝叶斯思路的结构模型计算算法
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{总结与展望-工作展望}
{\Large 展望}
\begin{itemize}
\item 实现已提出算法，并进行实验验证
\item 对试验中算法不同的参数进行测试，改进算法
\item 尝试寻找更多对排序有利的结构模型特征，验证并引入到最终的结构模型
\end{itemize}
\end{frame}

\section{附录}
\begin{frame}
\frametitle{概要}
\begin{block}
{\LARGE \tableofcontents[currentsection]}
\end{block}
\end{frame}

\begin{frame}[allowframebreaks]{进度安排}
\scriptsize
\baselineskip=20pt
2009年5月-2009年7月：完成了相关理论工作的调研，大量阅读相关论文文献；

2009年8月-2009年9月：搭建实验平台，复现已有的经典算法；

2009年10月-2009年11月：完成内容模型生成算法的设计与实现；

2009年12月-2010年1月：完成结构模型生成算法的设计与实现；

2010年2月-2010年3月：完成实验及结果评价。

\end{frame}

\begin{frame}[allowframebreaks]{论文大纲}
\scriptsize
\baselineskip=12pt
摘要

Abstract

第一章 绪论

\ \ 1.1 引言

\ \ 1.2 研究现状

\ \ 1.3 本文动因

\ \ 1.4 论文主要工作及目标

\ \ 1.5 论文组织结构

第二章 相关工作综述

\ \ 2.1 排序模型综述

\ \ 2.2 语言模型综述

第三章 内容模型生成算法

\ \ 3.1 网页噪声和多元组的影响

\ \ 3.2 基于变长多元组内容模型生成算法

\ \ 3.3 未见词的处理问题

\ \ 3.4 多层数据集平滑算法

第四章 结构模型生成算法

\ \ 4.1 文档结构知识对排序的影响

\ \ 4.2 文档先验概率的融合

\ \ 4.3 算法描述

第五章 实验结果及分析

\ \ 5.1 实验设计

\ \ 5.2 数据集

\ \ 5.3 评价指标

\ \ 5.4 实验流程

\ \ 5.5 实验结果及分析

第六章 结束语

\ \ 6.1 本文工作总结

\ \	6.2 未来工作展望

参考文献

致谢

\end{frame}

\begin{frame}[allowframebreaks]{参考文献}
\begin{thebibliography}{10}
\tiny
\bibitem{}
[C. Zhai and J. Lafferty]
\newblock A Study of Smoothing Methods for Language Models Applied to Information Retrieval
\newblock \emph{ACM Treansaction of Information System}, 2004.
\bibitem{}
[Miller D. H.]
\newblock A hidden Markov model information retrieval system
\newblock \emph{Proceedings of the ACM SIGIR}, 1999.
\bibitem{}
[C. Zhai and J. Lafferty]
\newblock A Study of Smoothing Methods for Language Models Applied to Information Retrieval
\newblock \emph{ACM Transaction of Information System}, 2004.
\bibitem{}
[Tom Mitchell]
\newblock Machine Learning
\newblock \emph{McGraw Hill}, 1997.
\bibitem{}
[Baeza-Yates, R. and Ribeiro-Neto B.]
\newblock Modern Information Retrieval
\newblock \emph{New York, NY, USA: Addition Wesley}, 1999.
\bibitem{}
[Lancaster F. W.]
\newblock Information retrieval systems: characteristics, testing and evaluation
\newblock \emph{2nd Ed., New York: John Wiely and Sons}, 1979.
\bibitem{}
[Salton, G., Wong, A., and Yang, C. S.]
\newblock A vector space model for automatic indexing
\newblock \emph{Communications of the ACM}, 1975.
\bibitem{}
[Robertson, S. E.]
\newblock Okapi in TREC3
\newblock \emph{NIST Special Publication}, 1994.
\bibitem{}
[J. Lafferty and C. Zhai]
\newblock Document Language Models, Query Models, and Risk Minimization for Information Retrieval
\newblock \emph{Proceedings of the 24th annual international ACM SIGIR conference}, 2001.
\bibitem{}
[Freund, Y., Iyer, R., Schapire, R., and Singer, Y.]
\newblock An efficient boosting algorithm for combining preferences
\newblock \emph{Journal of Machine Learning}, 2004.
\bibitem{}
[Nallapati, R.]
\newblock Discriminative Models for Information Retrieval
\newblock \emph{Proceedings of the27th Annual International ACM SIGIR Conference}, 2004.
\bibitem{}
[N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.]
\newblock Overview of the TREC 2003 web track
\newblock \emph{TREC}, 2003.
\bibitem{}
[Lemur Project]
\newblock http://www.lemurproject.org/
\bibitem{}
[Tie-Yan Liu, Jun Xu, Tao Qin, Wenying Xiong, and Hang Li]
\newblock LETOR: Benchmarking "Learning to Rank for Information Retrieval"
\newblock \emph{SIGIR 2007 Workshop on Learning to Rank for Information Retrieval}, 2007.
\bibitem{}
[F. Bimbot, R. Preraccini, E. Levin and B. Atal]
\newblock Variable-Length Sequence Modeling: Multigrams
\newblock \emph{IEEE Signal Processing Letter}, 1995.
\bibitem{}
[S. Deligne and F. Bimbot]
\newblock Language modeling by variable length sequences: Theoretical formulation and evaluation of multigrams
\newblock \emph{Acoustics, Speech and Signal Processing}, 1995.
\bibitem{}
[R. Kneser and V. Steinbiss]
\newblock On the dynamic adaption of stochastic language models
\newblock \emph{Acoustics, Speech and Signal Processing}, 1993.

\end{thebibliography}
\end{frame}

\begin{frame}[c]
\begin{center} \Huge 谢 谢 ! \end{center}
\end{frame}

\end{CJK}
\end{document} 